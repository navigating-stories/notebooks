{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Digital Story Grammar\n",
    "\n",
    "In this notebook, we aim to implement a version of Digital Story Grammar (DSG; Bastholm Andrade & Andersen; [link](https://www.tandfonline.com/doi/abs/10.1080/13645579.2020.1723205)) that works with multiple languages (in particular Dutch, German, Danish, and English). The code will interface to the spaCy NLP library that has pretrained and easy-to-use pipelines for many languages available.\n",
    "\n",
    "The goal of the method is to extract the subject (actor), main verb (action), and object of each sentence or phrase. In DSG, these are referenced as *narrative units* and enable the construction of character networks. This notebook will use spaCy's `DependencyMatcher` to extract patterns in dependency relations of each sentence. However, each language has somewhat different dependency relations so it requires a specific set of patterns to be matched. German in particular uses a completely different notation for dependency relations, so it requires entirely different patterns that the other three languages (see this [link](https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/annotation/tiger_scheme-syntax.pdf) for the notation scheme). \n",
    "\n",
    "The patterns for each language are store in a separate `multilingual_dsg_patterns_xx.json` file. The pattern files consist of a nested list of dictionaries `[[{}, {}, ...], ...]`. Each list of dictionaries `[{},{}, ...]` represents a pattern of dependency relations that will be matched. Each dictionary in the list represents a token that is part of the dependency relations pattern. The first dictionary in each pattern has usually two entries: `RIGHT_ID` indicating a name for the token in the pattern; and `RIGHT_ATTRS` listing the required attributes of the token for a match (e.g., `{\"DEP\": \"nsubj\"}` means that the token must be a subject in a phrase). There can be `OR` or `NOT` type matches by using `IN` or `NOT_IN` keys for dictionaries (see the pattern files for examples). Subsequent dictionaries in a pattern have two additional entries: `LEFT_ID` which refers to another token in the parsing tree to which the current token is related; and `REL_OP` specifying the relation between the other token and the token to be matched. An overview of possible relations is presented on https://spacy.io/api/dependencymatcher. For example `REL_OP: >` indicates a match when the `LEFT_ID` token is the head of the `RIGHT_ID` token in the parsing tree.\n",
    "\n",
    "Example:\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"RIGHT_ID\": \"verb\",\n",
    "    \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"verb\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "  },\n",
    "  {\n",
    "     \"LEFT_ID\": \"verb\",\n",
    "     \"REL_OP\": \">>\",\n",
    "     \"RIGHT_ID\": \"obj\",\n",
    "     \"RIGHT_ATTRS\": {\"DEP\": \"pobj\"}\n",
    "  }\n",
    "]\n",
    "\n",
    "if a word has POS tag VERB or AUX and\n",
    "   the word has a dependent (DEP) word with relation nsubj and\n",
    "   the word has a dependent (DEP) word with relation pobj \n",
    "then word1 is a verb, word2 is a subj and word3 is an obj \n",
    "```\n",
    "\n",
    "A sentence or phrase can have multiple matches for a pattern (i.e., multiple objects or conjuncts), and for each match a row is added to the output table.\n",
    "\n",
    "A step-by-step example can also be found on https://spacy.io/api/dependencymatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Multilingual Digital Story Grammar \"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import warnings\n",
    "# Change this to use different language as example; see spacy.io/models\n",
    "from spacy.lang.en.examples import sentences as en_sentences\n",
    "from spacy.lang.nl.examples import sentences as nl_sentences\n",
    "from spacy.lang.de.examples import sentences as de_sentences\n",
    "from spacy.lang.da.examples import sentences as da_sentences\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy import displacy\n",
    "\n",
    "if spacy.__version__ < \"3\":\n",
    "    warnings.warn(\n",
    "        \"Module 'spacy' should be version >= 3.0 to run this notebook without errors\")\n",
    "if pd.__version__ < \"1.0\":\n",
    "    warnings.warn(\n",
    "        \"Module 'pandas' should be version >= 1.0 to run this notebook without errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_SPACY_PIPELINE = \"en_core_web_sm\"\n",
    "EN_DEPENDENCY_PATTERN_FILE = \"multilingual_dsg_patterns_en.json\"\n",
    "\n",
    "NL_SPACY_PIPELINE = \"nl_core_news_sm\"\n",
    "NL_DEPENDENCY_PATTERN_FILE = \"multilingual_dsg_patterns_nl.json\"\n",
    "\n",
    "DE_SPACY_PIPELINE = \"de_core_news_sm\"\n",
    "DE_DEPENDENCY_PATTERN_FILE = \"multilingual_dsg_patterns_de.json\"\n",
    "\n",
    "DA_SPACY_PIPELINE = \"da_core_news_sm\"\n",
    "DA_DEPENDENCY_PATTERN_FILE = \"multilingual_dsg_patterns_da.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few test sentences\n",
    "examples = [\n",
    "    \"The bird flew over the roof.\",\n",
    "    \"The cow ate the grass. The goat watched the cow.\",\n",
    "    \"The cow ate the grass while the goat watched the cow.\",\n",
    "    \"The goat watched the cow which was eating grass.\",\n",
    "    \"The goat attempted eating the cow's grass.\",\n",
    "    \"The cow ate grass, the cow ate butter.\",\n",
    "    \"The cow and the goat ate grass.\",\n",
    "    \"The grass was eaten by the cow, the goat and the bird.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_pipeline(name):\n",
    "    \"\"\"Check if the spacy language pipeline was downloaded and load it.\n",
    "    Downloads the language pipeline if not available.\n",
    "\n",
    "    Args:\n",
    "        name (string): Name of the spacy language.\n",
    "\n",
    "    Returns:\n",
    "        spacy.language.Language: The spacy language pipeline\n",
    "    \"\"\"\n",
    "    if spacy.util.is_package(name):\n",
    "        nlp = spacy.load(name)\n",
    "    else:\n",
    "        os.system(f\"spacy download {name}\")\n",
    "        nlp = spacy.load(name)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict_in_list(dict_obj, dict_list):\n",
    "    \"\"\"Check if a dictionary (partially) matches a list of dictionaries.\n",
    "\n",
    "    Note: This function is used to avoid duplicate matches (e.g., Subj+Verb in Subj+Verb+Obj)\n",
    "\n",
    "    Args:\n",
    "        dict_obj (dict): A dictionary object.\n",
    "        dict_list (list): A list of dictionary objects.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all non-empty items in dict_obj match the items in any dictionary objects in dict_list, otherwise False.\n",
    "    \"\"\"\n",
    "    if dict_obj in dict_list:\n",
    "        return True\n",
    "\n",
    "    check = [False] * len(dict_obj.keys())\n",
    "\n",
    "    for i, key in enumerate(dict_obj.keys()):\n",
    "        if str(dict_obj[key]) == \"_\":\n",
    "            check[i] = True\n",
    "            next\n",
    "        else:\n",
    "            for ref_dict in dict_list:\n",
    "                if dict_obj[key].i == ref_dict[key].i:\n",
    "                    check[i] = True\n",
    "                    break\n",
    "\n",
    "    return all(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matches(doc, matches, matcher, nlp, keys):\n",
    "    \"\"\"Extract the matched tokens for selected keys.\n",
    "\n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): A spacy doc object as returned by a spacy language pipeline.\n",
    "        matches (list): A list of (match_id, token_ids) tuples as returned by a spacy dependency matcher.\n",
    "        matcher (spacy.matcher.DependencyMatcher): A spacy dependency matcher object.\n",
    "        nlp (spacy.language.Language): A spacy language pipeline.\n",
    "        keys (list): A list of keys to which the dependcy matches are assigned.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries that each contain a match of the dependency matcher. \n",
    "            Has the same keys as the `keys` argument. Empty keys contain a spacy token with text='_'.\n",
    "    \"\"\"\n",
    "    matches_list = []\n",
    "\n",
    "    for l, (match_id, token_ids) in enumerate(matches):\n",
    "        match_dict = {}\n",
    "\n",
    "        for key in keys:\n",
    "            match_dict[key] = nlp(\"_\")[0]\n",
    "\n",
    "        for k, token_id in enumerate(token_ids):\n",
    "            key = matcher.get(match_id)[1][0][k][\"RIGHT_ID\"]\n",
    "            if key in match_dict.keys():\n",
    "                match_dict[key] = doc[token_id]\n",
    "\n",
    "        if not check_dict_in_list(match_dict, matches_list):\n",
    "            match_dict[\"match_id\"] = match_id\n",
    "            matches_list.append(match_dict)\n",
    "\n",
    "    return matches_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matcher(nlp, pattern_file):\n",
    "    \"\"\"Create a spacy dependency matcher.\n",
    "\n",
    "    Args:\n",
    "        nlp (spacy.language.Language): A spacy language pipeline.\n",
    "        pattern_file (str): The path to the dependency pattern .json file for the matcher.\n",
    "\n",
    "    Returns:\n",
    "        spacy.matcher.DependencyMatcher: A spacy dependency matcher object.\n",
    "    \"\"\"\n",
    "    matcher = DependencyMatcher(nlp.vocab, validate=True)\n",
    "\n",
    "    with open(pattern_file, \"r\") as file:\n",
    "        patterns = json.load(file)\n",
    "\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        matcher.add(i, [pattern])\n",
    "\n",
    "    return matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_children_deps(token, doc, children_deps):\n",
    "    \"\"\"Append children to a token based on dependency tag.\n",
    "\n",
    "    Note: This function is used to append words of a noun compound.\n",
    "\n",
    "    Args:\n",
    "        token (spacy.token.Token): A spacy token object.\n",
    "        doc (spacy.token.Doc): A spacy doc object that includes the token.\n",
    "        children_deps (list): A list of dependency tags.\n",
    "\n",
    "    Returns:\n",
    "        spacy.token.Token: A span of spacy tokens (token argument plus children with specified dependency tags) if token argument is non-empty, the token argument otherwise.\n",
    "    \"\"\"\n",
    "    if str(token) != \"_\":\n",
    "        children_match_idx = [\n",
    "            child.i for child in token.children if child.dep_ in children_deps] + [token.i]\n",
    "\n",
    "        span = doc[min(children_match_idx):max(children_match_idx)+1]\n",
    "\n",
    "        return span\n",
    "    else:\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_object_verb_table(docs, nlp, matcher, keys=[\"verb\", \"subj\", \"obj\", \"comp\"]):\n",
    "    \"\"\"Construct a pandas dataframe with subjects, verbs, and objects per sentence of documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of text strings.\n",
    "        nlp (spacy.language.Language): A spacy language pipeline.\n",
    "        matcher (spacy.matcher.DependencyMatcher): A spacy dependency matcher object.\n",
    "        nlp (spacy.language.Language): A spacy language pipeline.\n",
    "        keys (list): A list of keys to which the dependcy matches are assigned. \n",
    "            Defaults to subjects, verbs, and objects.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with a row for each match of the dependency matcher and cols:\n",
    "            doc_id (str): Index of the document in the document list.\n",
    "            sent_id (str): Index of the sentence in the document.\n",
    "            sent (spacy.tokens.Span): A spacy span object with the sentence.\n",
    "            match_id (str): Index of the match in the sentence.\n",
    "\n",
    "            For each key in the `keys` argument:\n",
    "                key (spacy.tokens.Token): A spacy token object that matches the dependency matcher patterns.\n",
    "    \"\"\"\n",
    "    docs_piped = nlp.pipe(docs)\n",
    "\n",
    "    table_dict = {\n",
    "        \"doc_id\": [],\n",
    "        \"sent_id\": [],\n",
    "        \"sent\": [],\n",
    "        \"match_id\": [],\n",
    "        \"subj\": [],\n",
    "        \"verb\": [],\n",
    "        \"obj\": [],\n",
    "        \"comp\": [],\n",
    "    }\n",
    "\n",
    "    for i, doc in enumerate(docs_piped): # i: doc index\n",
    "        for j, sent in enumerate(doc.sents): # j: sent index\n",
    "\n",
    "            matches = matcher(sent)\n",
    "            matches_list = extract_matches(\n",
    "                sent, matches, matcher, nlp, keys=keys)\n",
    "            for l, match in enumerate(matches_list): # l: match index\n",
    "                table_dict[\"doc_id\"].append(str(i))\n",
    "                table_dict[\"sent_id\"].append(str(j))\n",
    "                table_dict[\"sent\"].append(sent.text)\n",
    "                table_dict[\"match_id\"].append(str(match[\"match_id\"]))\n",
    "\n",
    "                for key in keys:\n",
    "                    table_dict[key].append(append_children_deps(\n",
    "                        match[key], doc, [\"compound\", \"flat\"]))\n",
    "\n",
    "                    # Check for conjuncts, and add table row for each\n",
    "                    for conj in match[key].conjuncts:\n",
    "                        table_dict[\"doc_id\"].append(str(i))\n",
    "                        table_dict[\"sent_id\"].append(str(j))\n",
    "                        table_dict[\"sent\"].append(sent.text)\n",
    "                        table_dict[\"match_id\"].append(str(\"?\"))\n",
    "                        table_dict[key].append(conj)\n",
    "                        for key_conj in keys:\n",
    "                            if key != key_conj:\n",
    "                                table_dict[key_conj].append(\n",
    "                                    match[key_conj])\n",
    "    for i in range(0, len(table_dict[\"comp\"])):\n",
    "        # insert table_dict[\"comp\"][i] in table_dict[\"verb\"][i]) here\n",
    "        pass\n",
    "    \n",
    "    return pd.DataFrame(table_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character_network(sov_table_df):\n",
    "    \"\"\"Plots a subject-object-verb table as a character network. The edge weights are the sentiment scores of the verbs.\n",
    "\n",
    "    Args:\n",
    "        sov_table_df (pandas.DataFrame): A pandas data frame containing a subject, verb, and object token in each row\n",
    "\n",
    "    \"\"\"\n",
    "    sov_table_df[\"sentiment\"] = pd.Series([verb.sentiment for verb in sov_table_df[\"verb\"]], dtype=float)\n",
    "    sov_table_df[\"subj_text\"] = pd.Series([subj.text for subj in sov_table_df[\"subj\"]], dtype=str)\n",
    "    sov_table_df[\"obj_text\"] = pd.Series([obj.text for obj in sov_table_df[\"obj\"]], dtype=str)\n",
    "\n",
    "    char_net = nx.from_pandas_edgelist(\n",
    "        sov_table_df,\n",
    "        source=\"subj_text\",\n",
    "        target=\"obj_text\",\n",
    "        edge_attr=\"sentiment\"\n",
    "    )\n",
    "    \n",
    "    nx.draw_networkx(char_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple is looking at buying U.K. startup for $1 billion',\n",
       " 'Autonomous cars shift insurance liability toward manufacturers',\n",
       " 'San Francisco considers banning sidewalk delivery robots',\n",
       " 'London is a big city in the United Kingdom.',\n",
       " 'Where are you?',\n",
       " 'Who is the president of France?',\n",
       " 'What is the capital of the United States?',\n",
       " 'When was Barack Obama born?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en = load_spacy_pipeline(EN_SPACY_PIPELINE)\n",
    "\n",
    "matcher_en = create_matcher(nlp_en, EN_DEPENDENCY_PATTERN_FILE)\n",
    "\n",
    "en_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sent</th>\n",
       "      <th>match_id</th>\n",
       "      <th>subj</th>\n",
       "      <th>verb</th>\n",
       "      <th>obj</th>\n",
       "      <th>comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Apple)</td>\n",
       "      <td>(looking)</td>\n",
       "      <td>(U.K.)</td>\n",
       "      <td>(buying)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Apple)</td>\n",
       "      <td>(looking)</td>\n",
       "      <td>(1, billion)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autonomous cars shift insurance liability towa...</td>\n",
       "      <td>0</td>\n",
       "      <td>(cars)</td>\n",
       "      <td>(shift)</td>\n",
       "      <td>(insurance, liability)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autonomous cars shift insurance liability towa...</td>\n",
       "      <td>3</td>\n",
       "      <td>(cars)</td>\n",
       "      <td>(shift)</td>\n",
       "      <td>(manufacturers)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>San Francisco considers banning sidewalk deliv...</td>\n",
       "      <td>2</td>\n",
       "      <td>(San, Francisco)</td>\n",
       "      <td>(considers)</td>\n",
       "      <td>(delivery, robots)</td>\n",
       "      <td>(banning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>London is a big city in the United Kingdom.</td>\n",
       "      <td>3</td>\n",
       "      <td>(London)</td>\n",
       "      <td>(is)</td>\n",
       "      <td>(United, Kingdom)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Where are you?</td>\n",
       "      <td>4</td>\n",
       "      <td>(you)</td>\n",
       "      <td>(are)</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the capital of the United States?</td>\n",
       "      <td>3</td>\n",
       "      <td>(capital)</td>\n",
       "      <td>(is)</td>\n",
       "      <td>(United, States)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>When was Barack Obama born?</td>\n",
       "      <td>4</td>\n",
       "      <td>(Barack, Obama)</td>\n",
       "      <td>(born)</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id sent_id                                               sent match_id  \\\n",
       "0      0       0  Apple is looking at buying U.K. startup for $1...        2   \n",
       "1      0       0  Apple is looking at buying U.K. startup for $1...        3   \n",
       "2      1       0  Autonomous cars shift insurance liability towa...        0   \n",
       "3      1       0  Autonomous cars shift insurance liability towa...        3   \n",
       "4      2       0  San Francisco considers banning sidewalk deliv...        2   \n",
       "5      3       0        London is a big city in the United Kingdom.        3   \n",
       "6      4       0                                     Where are you?        4   \n",
       "7      6       0          What is the capital of the United States?        3   \n",
       "8      7       0                        When was Barack Obama born?        4   \n",
       "\n",
       "               subj         verb                     obj       comp  \n",
       "0           (Apple)    (looking)                  (U.K.)   (buying)  \n",
       "1           (Apple)    (looking)            (1, billion)          _  \n",
       "2            (cars)      (shift)  (insurance, liability)          _  \n",
       "3            (cars)      (shift)         (manufacturers)          _  \n",
       "4  (San, Francisco)  (considers)      (delivery, robots)  (banning)  \n",
       "5          (London)         (is)       (United, Kingdom)          _  \n",
       "6             (you)        (are)                       _          _  \n",
       "7         (capital)         (is)        (United, States)          _  \n",
       "8   (Barack, Obama)       (born)                       _          _  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_object_verb_table(en_sentences, nlp_en, matcher_en)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sov_table_en = get_subject_object_verb_table(examples, nlp_en, matcher_en)\n",
    "print(sov_table_en)\n",
    "plot_character_network(sov_table_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_nl = load_spacy_pipeline(NL_SPACY_PIPELINE)\n",
    "\n",
    "matcher_nl = create_matcher(nlp_nl, NL_DEPENDENCY_PATTERN_FILE)\n",
    "\n",
    "nl_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subject_object_verb_table(nl_sentences, nlp_nl, matcher_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp_nl(nl_sentences[0]), style=\"dep\", options={\"distance\": 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = load_spacy_pipeline(DE_SPACY_PIPELINE)\n",
    "\n",
    "matcher_de = create_matcher(nlp_de, DE_DEPENDENCY_PATTERN_FILE)\n",
    "\n",
    "de_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subject_object_verb_table(de_sentences, nlp_de, matcher_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp_de(de_sentences[0]), style=\"dep\", options={\"distance\": 75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the results of the dependency matcher and the parsing tree, it can be seen that the parser makes a few mistakes: In the first sentence, it erroneously labels \"Silicon Valley\" as a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_da = load_spacy_pipeline(DA_SPACY_PIPELINE)\n",
    "\n",
    "matcher_da = create_matcher(nlp_da, DA_DEPENDENCY_PATTERN_FILE)\n",
    "\n",
    "da_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subject_object_verb_table(da_sentences, nlp_da, matcher_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp_da(da_sentences[2]), style=\"dep\", options={\"distance\": 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Danish parser also makes a few mistakes: For example, in the second sentence it labels both \"San\" and \"Francisco\" as separate subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
